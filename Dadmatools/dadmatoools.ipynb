{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dadmatools in c:\\users\\home\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: supar==1.1.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.1.2)\n",
      "Requirement already satisfied: py7zr>=0.17.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.20.0)\n",
      "Requirement already satisfied: NERDA in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.0.0)\n",
      "Requirement already satisfied: conllu in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (4.5.2)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.5.11)\n",
      "Requirement already satisfied: gdown>=4.3.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (4.5.1)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tabulate>=0.8.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.8.9)\n",
      "Requirement already satisfied: html2text in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (2020.1.16)\n",
      "Requirement already satisfied: torch>=1.7.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.12.1)\n",
      "Requirement already satisfied: hyperopt>=0.2.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.2.7)\n",
      "Requirement already satisfied: spacy>=3.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (3.4.1)\n",
      "Requirement already satisfied: h5py>=3.3.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (3.6.0)\n",
      "Requirement already satisfied: pytorch-transformers>=1.1.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.2.0)\n",
      "Requirement already satisfied: bpemb>=0.3.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.3.3)\n",
      "Requirement already satisfied: transformers>=4.9.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (4.21.3)\n",
      "Requirement already satisfied: Deprecated==1.2.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (1.2.6)\n",
      "Requirement already satisfied: folium>=0.2.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.12.1.post1)\n",
      "Requirement already satisfied: gensim>=3.6.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (4.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (3.7)\n",
      "Requirement already satisfied: pyconll>=3.1.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (3.1.0)\n",
      "Requirement already satisfied: sklearn>=0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from dadmatools) (0.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\home\\anaconda3\\lib\\site-packages (from Deprecated==1.2.6->dadmatools) (1.12.1)\n",
      "Requirement already satisfied: stanza in c:\\users\\home\\anaconda3\\lib\\site-packages (from supar==1.1.2->dadmatools) (1.4.0)\n",
      "Requirement already satisfied: dill in c:\\users\\home\\anaconda3\\lib\\site-packages (from supar==1.1.2->dadmatools) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\home\\anaconda3\\lib\\site-packages (from bpemb>=0.3.3->dadmatools) (4.64.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\home\\anaconda3\\lib\\site-packages (from bpemb>=0.3.3->dadmatools) (0.1.97)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\lib\\site-packages (from bpemb>=0.3.3->dadmatools) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\home\\anaconda3\\lib\\site-packages (from bpemb>=0.3.3->dadmatools) (2.27.1)\n",
      "Requirement already satisfied: branca>=0.3.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from folium>=0.2.1->dadmatools) (0.5.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\home\\anaconda3\\lib\\site-packages (from folium>=0.2.1->dadmatools) (2.11.3)\n",
      "Requirement already satisfied: six in c:\\users\\home\\anaconda3\\lib\\site-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\lib\\site-packages (from gdown>=4.3.1->dadmatools) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from gdown>=4.3.1->dadmatools) (4.11.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from gensim>=3.6.0->dadmatools) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from gensim>=3.6.0->dadmatools) (5.2.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.5->dadmatools) (2.7.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\home\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.5->dadmatools) (2.0.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\home\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n",
      "Requirement already satisfied: future in c:\\users\\home\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.5->dadmatools) (0.18.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\home\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (5.8.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (0.15.3)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (1.0.1)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: brotli>=1.0.9 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (1.0.9)\n",
      "\n",
      "Requirement already satisfied: inflate64>=0.3.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (0.3.0)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (0.2.3)\n",
      "Requirement already satisfied: pyppmd<0.19.0,>=0.18.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (0.18.3)\n",
      "Requirement already satisfied: texttable in c:\\users\\home\\anaconda3\\lib\\site-packages (from py7zr>=0.17.2->dadmatools) (1.6.4)\n",
      "Requirement already satisfied: regex in c:\\users\\home\\anaconda3\\lib\\site-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.3.15)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\home\\anaconda3\\lib\\site-packages (from pytorch-transformers>=1.1.0->dadmatools) (0.0.53)\n",
      "Requirement already satisfied: boto3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from pytorch-transformers>=1.1.0->dadmatools) (1.21.32)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\home\\anaconda3\\lib\\site-packages (from sklearn>=0.0->dadmatools) (1.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (2.4.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (3.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (1.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (1.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (3.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->dadmatools) (8.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=3.0.0->dadmatools) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy>=3.0.0->dadmatools) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.3->dadmatools) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.3->dadmatools) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.3->dadmatools) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\home\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\lib\\site-packages (from tqdm->bpemb>=0.3.3->dadmatools) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers>=4.9.1->dadmatools) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers>=4.9.1->dadmatools) (0.9.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->dadmatools) (8.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.3.1)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in c:\\users\\home\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (1.24.32)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3->pytorch-transformers>=1.1.0->dadmatools) (2.8.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\home\\anaconda3\\lib\\site-packages (from NERDA->dadmatools) (1.4.2)\n",
      "Requirement already satisfied: progressbar in c:\\users\\home\\anaconda3\\lib\\site-packages (from NERDA->dadmatools) (2.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\home\\anaconda3\\lib\\site-packages (from nltk->dadmatools) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from pandas->NERDA->dadmatools) (2021.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn>=0.0->dadmatools) (2.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\home\\anaconda3\\lib\\site-packages (from stanza->supar==1.1.2->dadmatools) (3.19.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\home\\anaconda3\\lib\\site-packages (from stanza->supar==1.1.2->dadmatools) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install dadmatools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text :  \n",
      "<p>\n",
      "دادماتولز اولین نسخش سال ۱۴۰۰ منتشر شده. \n",
      "امیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه\n",
      "لطفا با ایمیل dadmatools@dadmatech.ir با ما در ارتباط باشید\n",
      "آدرس گیت‌هاب هم که خب معرف حضور مبارک هست:\n",
      " https://github.com/Dadmatech/DadmaTools\n",
      "</p>\n",
      "\n",
      "output text when replace emails and remove urls :  <p> دادماتولز اولین نسخش سال 1400 منتشر شده. امیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه لطفا با ایمیل <EMAIL> با ما در ارتباط باشید آدرس گیت‌هاب هم که خب معرف حضور مبارک هست: </p>\n",
      "output text when using full_cleaning parameter دادماتولز اولین نسخش سال منتشر شده امیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه لطفا با ایمیل با ما در ارتباط باشید آدرس گیت‌هاب هم که خب معرف حضور مبارک هست\n"
     ]
    }
   ],
   "source": [
    "from dadmatools.models.normalizer import Normalizer\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    full_cleaning=False,\n",
    "    unify_chars=True,\n",
    "    refine_punc_spacing=True,\n",
    "    remove_extra_space=True,\n",
    "    remove_puncs=False,\n",
    "    remove_html=False,\n",
    "    remove_stop_word=False,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=None,\n",
    "    replace_url_with=\"\",\n",
    "    replace_mobile_number_with=None,\n",
    "    replace_emoji_with=None,\n",
    "    replace_home_number_with=None\n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "<p>\n",
    "دادماتولز اولین نسخش سال ۱۴۰۰ منتشر شده. \n",
    "امیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه\n",
    "لطفا با ایمیل dadmatools@dadmatech.ir با ما در ارتباط باشید\n",
    "آدرس گیت‌هاب هم که خب معرف حضور مبارک هست:\n",
    " https://github.com/Dadmatech/DadmaTools\n",
    "</p>\n",
    "\"\"\"\n",
    "print('input text : ', text)\n",
    "print('output text when replace emails and remove urls : ', normalizer.normalize(text))\n",
    "\n",
    "#full cleaning\n",
    "normalizer = Normalizer(full_cleaning=True)\n",
    "print('output text when using full_cleaning parameter', normalizer.normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dadmatools.datasets import get_all_datasets_info, get_dataset_info\n",
    "from dadmatools.datasets import ARMAN\n",
    "from dadmatools.datasets import TEP\n",
    "from dadmatools.datasets import PerSentLexicon\n",
    "from dadmatools.datasets import FaSpell\n",
    "from dadmatools.datasets import WikipediaCorpus\n",
    "from dadmatools.datasets import PersianNer\n",
    "from dadmatools.datasets import PersianNews\n",
    "from dadmatools.datasets import PnSummary\n",
    "from dadmatools.datasets import FarsTail\n",
    "from dadmatools.datasets import SnappfoodSentiment\n",
    "from dadmatools.datasets import get_all_datasets_info\n",
    "from dadmatools.datasets import Peyma\n",
    "from dadmatools.datasets import PerUDT\n",
    "from dadmatools.datasets import PersianTweets\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARMAN': {'description': 'ARMAN dataset holds 7,682 sentences with 250,015 '\n",
      "                          'sentences tagged over six different classes.\\n'\n",
      "                          '\\n'\n",
      "                          'Organization\\n'\n",
      "                          'Location\\n'\n",
      "                          'Facility\\n'\n",
      "                          'Event\\n'\n",
      "                          'Product\\n'\n",
      "                          'Person',\n",
      "           'filenames': ['train_fold1.txt',\n",
      "                         'train_fold2.txt',\n",
      "                         'train_fold3.txt',\n",
      "                         'test_fold1.txt',\n",
      "                         'test_fold2.txt',\n",
      "                         'test_fold3.txt'],\n",
      "           'name': 'ARMAN',\n",
      "           'size': {'test': 7680, 'train': 15361},\n",
      "           'splits': ['train', 'test'],\n",
      "           'task': 'NER',\n",
      "           'version': '1.0.0'},\n",
      " 'PersianNer': {'description': 'source: '\n",
      "                               'https://github.com/Text-Mining/Persian-NER',\n",
      "                'filenames': ['Persian-NER-part1.txt',\n",
      "                              'Persian-NER-part2.txt',\n",
      "                              'Persian-NER-part3.txt',\n",
      "                              'Persian-NER-part4.txt',\n",
      "                              'Persian-NER-part5.txt'],\n",
      "                'name': 'PersianNer',\n",
      "                'size': 976599,\n",
      "                'splits': [],\n",
      "                'task': 'NER',\n",
      "                'version': '1.0.0'},\n",
      " 'Peyma': {'description': 'source: '\n",
      "                          'http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/',\n",
      "           'filenames': ['peyma/600K', 'peyma/300K'],\n",
      "           'name': 'Peyma',\n",
      "           'size': 10016,\n",
      "           'splits': [],\n",
      "           'task': 'NER',\n",
      "           'version': '1.0.0'},\n",
      " 'snappfoodSentiment': {'description': 'source: '\n",
      "                                       'https://huggingface.co/HooshvareLab/bert-fa-base-uncased-sentiment-snappfood',\n",
      "                        'filenames': ['snappfood/train.csv',\n",
      "                                      'snappfood/test.csv',\n",
      "                                      'snappfood/dev.csv'],\n",
      "                        'name': 'snappfoodSentiment',\n",
      "                        'size': {'dev': 6274, 'test': 6972, 'train': 56516},\n",
      "                        'splits': ['train', 'test', 'dev'],\n",
      "                        'task': 'Sentiment-Analysis',\n",
      "                        'version': '1.0.0'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(get_all_datasets_info(tasks=['NER', 'Sentiment-Analysis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'source: https://github.com/dml-qom/FarsTail',\n",
      " 'filenames': ['Train-word.csv', 'Test-word.csv', 'Val-word.csv'],\n",
      " 'name': 'FarsTail',\n",
      " 'size': {'test': 1564, 'train': 7266, 'val': 1537},\n",
      " 'splits': ['train', 'test', 'val'],\n",
      " 'task': 'Textual-Entailment',\n",
      " 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "pprint(get_dataset_info('FarsTail'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** WikipediaCorpus dataset ****\n",
      "\n",
      "len data  2184117\n",
      "\n",
      "sample:  {'title': 'طµظپط\\xadظ‡ظ” ط§طµظ„غŒ', 'content': '&nbsp;ظ…ظ‚ط§ظ„ظ‡â€Œظ‡ط§غŒ ط¨ط±ع¯ط²غŒط¯ظ‡ &ndash; ظ…ظ‚ط§ظ„ظ‡ظ” ط§ظ…ط±ظˆط²\\nط§ظ…ط±ظˆط²: طŒ  ظ…غŒظ„ط§ط¯غŒ ط¨ط±ط§ط¨ط±  ظ‡ط¬ط±غŒ ط®ظˆط±ط´غŒط¯غŒ ظˆ  (UTC)\\nâ†’ ط±ظˆط² ظ‚ط¨ظ„ &ndash; ط±ظˆط² ط¨ط¹ط¯ â†گغŒط§ط¯ط¨ظˆط¯ظ‡ط§غŒ  &ndash; غŒط§ط¯ط¨ظˆط¯ظ‡ط§غŒ ط¨غŒط´طھط±...\\n ط¨ط§غŒع¯ط§ظ†غŒ   &ndash; ظ†ع¯ط§ط±ظ‡â€Œظ‡ط§غŒ ط¨ط±ع¯ط²غŒط¯ظ‡ظ” ط¨غŒط´طھط±\\n __NOTOC__ __NOEDITSECTION__'}\n",
      "\n",
      "****** dataset details:********\n",
      " \n",
      "name: WikipediaCorpus\n",
      "version: 20211201\n",
      "task: Corpus\n",
      "description: fawiki dump progress on 20211201 / All pages, current versions only.\n",
      "size: 2184117\n",
      "filenames: ['cleaned_wiki.txt']\n"
     ]
    }
   ],
   "source": [
    "print('*** WikipediaCorpus dataset ****')\n",
    "print()\n",
    "wiki = WikipediaCorpus()\n",
    "print('len data ', len(wiki.data))\n",
    "print()\n",
    "print('sample: ', next(wiki.data))\n",
    "print()\n",
    "print('****** dataset details:********\\n ')\n",
    "print(wiki.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Peyma': {'name': 'Peyma',\n",
       "  'version': '1.0.0',\n",
       "  'task': 'NER',\n",
       "  'splits': [],\n",
       "  'description': 'source: http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/',\n",
       "  'size': 10016,\n",
       "  'filenames': ['peyma/600K', 'peyma/300K']},\n",
       " 'snappfoodSentiment': {'name': 'snappfoodSentiment',\n",
       "  'version': '1.0.0',\n",
       "  'task': 'Sentiment-Analysis',\n",
       "  'splits': ['train', 'test', 'dev'],\n",
       "  'description': 'source: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-sentiment-snappfood',\n",
       "  'size': {'train': 56516, 'dev': 6274, 'test': 6972},\n",
       "  'filenames': ['snappfood/train.csv',\n",
       "   'snappfood/test.csv',\n",
       "   'snappfood/dev.csv']},\n",
       " 'PersianNer': {'name': 'PersianNer',\n",
       "  'version': '1.0.0',\n",
       "  'task': 'NER',\n",
       "  'splits': [],\n",
       "  'description': 'source: https://github.com/Text-Mining/Persian-NER',\n",
       "  'size': 976599,\n",
       "  'filenames': ['Persian-NER-part1.txt',\n",
       "   'Persian-NER-part2.txt',\n",
       "   'Persian-NER-part3.txt',\n",
       "   'Persian-NER-part4.txt',\n",
       "   'Persian-NER-part5.txt']},\n",
       " 'ARMAN': {'name': 'ARMAN',\n",
       "  'version': '1.0.0',\n",
       "  'task': 'NER',\n",
       "  'splits': ['train', 'test'],\n",
       "  'description': 'ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes.\\n\\nOrganization\\nLocation\\nFacility\\nEvent\\nProduct\\nPerson',\n",
       "  'filenames': ['train_fold1.txt',\n",
       "   'train_fold2.txt',\n",
       "   'train_fold3.txt',\n",
       "   'test_fold1.txt',\n",
       "   'test_fold2.txt',\n",
       "   'test_fold3.txt'],\n",
       "  'size': {'train': 15361, 'test': 7680}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dadmatools.datasets import get_all_datasets_info\n",
    "\n",
    "get_all_datasets_info().keys()\n",
    "#dict_keys(['Persian-NEWS', 'fa-wiki', 'faspell', 'PnSummary', 'TEP', 'PerUDT', 'FarsTail', 'Peyma', 'snappfoodSentiment', 'Persian-NER', 'Arman', 'PerSent'])\n",
    "\n",
    "#specify task\n",
    "get_all_datasets_info(tasks=['NER', 'Sentiment-Analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Arman dataset **** \n",
      "splits:  ['train', 'test']\n",
      "15361\n",
      "[{'token': 'ط§ظپظ‚غŒ', 'tag': 'O'}, {'token': ':', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ط¹ظˆط§ظ…ظ„', 'tag': 'O'}, {'token': 'ط¯ظˆط±ط§ظ†', 'tag': 'O'}, {'token': 'ظ¾ظ‡ظ„ظˆغŒ', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ†ط®ط³طھâ€Œظˆط²غŒط±', 'tag': 'O'}, {'token': 'ط§غŒط±ط§ظ†', 'tag': 'B-loc'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط³ط§ظ„ظ‡ط§غŒ', 'tag': 'O'}, {'token': 'ط§ط¨طھط¯ط§ط¦غŒ', 'tag': 'O'}, {'token': 'ط¯ظ‡ظ‡', 'tag': 'O'}, {'token': 'ع†ظ‡ظ„', 'tag': 'O'}, {'token': 'ط®ظˆط±ط´غŒط¯غŒ', 'tag': 'O'}, {'token': 'ظƒظ‡', 'tag': 'O'}, {'token': 'ط¬ظ„ط¯', 'tag': 'O'}, {'token': 'ط³ظˆظ…', 'tag': 'O'}, {'token': 'غŒط§ط¯ط¯ط§ط´طھظ‡ط§غŒط´', 'tag': 'O'}, {'token': 'ظ‡ظ…', 'tag': 'O'}, {'token': 'ع†ظ†ط¯غŒ', 'tag': 'O'}, {'token': 'ظ¾غŒط´', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'طھظ‡ط±ط§ظ†', 'tag': 'B-loc'}, {'token': 'ظ…ظ†طھط´ط±', 'tag': 'O'}, {'token': 'ط´ط¯', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾ط±ط³طھط§ط±غŒ', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ظ†ط§ط®ظˆط´â€Œط§ط\\xadظˆط§ظ„', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾ظˆط´ط§ع©', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط¬ط§ظ…ظ‡', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظپط§ظ†طھط²غŒ', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط´غŒع©', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط\\xadط§ظ„', 'tag': 'O'}, {'token': 'ظˆط²غŒط¯ظ†', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط§ط·ظ„ط§ط¹غŒظ‡', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾ط§غŒطھط®طھ', 'tag': 'O'}, {'token': 'ط¬ظ…ظ‡ظˆط±غŒ', 'tag': 'O'}, {'token': 'ط§ط³طھظˆظ†غŒ', 'tag': 'B-loc'}, {'token': 'ط¯ط±', 'tag': 'I-loc'}, {'token': 'ط\\xadظˆط¶ظ‡', 'tag': 'I-loc'}, {'token': 'ط¨ط§ظ„طھغŒع©', 'tag': 'I-loc'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¹ظ„ظ…', 'tag': 'O'}, {'token': 'ط±ط§ظ‡ط¨ط±ط¯', 'tag': 'O'}, {'token': 'ظ…ط¤ط³ط³ظ‡', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط³ط§ط²ظ…ط§ظ†', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ†ظˆط¹غŒ', 'tag': 'O'}, {'token': 'ط´ظ…ط¹', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط\\xadط±ظپ', 'tag': 'O'}, {'token': 'ط¬ظ…ط¹', 'tag': 'O'}, {'token': 'ظ…ط¤ظ†ط«', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط§غŒط±ط§ظ†', 'tag': 'B-loc'}, {'token': 'ط¨ظ‡', 'tag': 'O'}, {'token': 'طھظˆظ„غŒط¯ع©ظ†ظ†ط¯ظ‡', 'tag': 'O'}, {'token': 'ع©طھط§ط¨', 'tag': 'O'}, {'token': 'ط§ط·ظ„ط§ظ‚', 'tag': 'O'}, {'token': 'ظ…غŒâ€Œط´ظˆط¯', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ط´ظ‡ط±ظ‡ط§غŒ', 'tag': 'O'}, {'token': 'ط¨ط§ط®طھط±غŒ', 'tag': 'O'}, {'token': 'ط§ظپط؛ط§ظ†ط³طھط§ظ†', 'tag': 'B-loc'}, {'token': 'ظƒظ‡', 'tag': 'O'}, {'token': 'طھط§', 'tag': 'O'}, {'token': 'ط¹طµط±', 'tag': 'O'}, {'token': 'ظ†ط§طµط±ط§ظ„ط¯غŒظ†â€Œط´ط§ظ‡', 'tag': 'B-pers'}, {'token': 'ط¬ط²ط¦غŒ', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ط®ط±ط§ط³ط§ظ†', 'tag': 'B-loc'}, {'token': 'ط¨ظˆط¯', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظˆغŒطھط§ظ…غŒظ†', 'tag': 'O'}, {'token': 'ط§ظ†ط¹ظ‚ط§ط¯', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط³ط¨ط²غŒ', 'tag': 'O'}, {'token': 'ط؛ط¯ظ‡â€Œط§غŒ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ظˆط³طھغŒ', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ…ط\\xadط¨طھ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ط§ط³طھط§ظ†', 'tag': 'O'}, {'token': 'ط¨ظ„ظ†ط¯', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط´ظ‡ط±غŒ', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط¢ظ„ظ…ط§ظ†', 'tag': 'B-loc'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط³ظ„ظˆظ„', 'tag': 'O'}, {'token': 'ط¨ط¯ظ†', 'tag': 'O'}, {'token': 'ظ…ظˆط¬ظˆط¯ط§طھ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ط§ظ†ظˆط§ط¹', 'tag': 'O'}, {'token': 'ع©ط§ظ„ط¨ط§ط³', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط\\xadط§ط´غŒظ‡', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ‡ط§ظ…ط´', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾غŒط¯ط§', 'tag': 'O'}, {'token': 'ظ†ط´ط¯ظ†غŒ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط®ط±ع¯ظˆط´', 'tag': 'O'}, {'token': 'طھط§ط²غŒ', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ع©ظ†ط§غŒظ‡', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ط¬ظ…ط¹â€Œط¢ظˆط±غŒ', 'tag': 'O'}, {'token': 'ط§ظپط±ط§ط·غŒ', 'tag': 'O'}, {'token': 'ظ…ط§ظ„', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط§ظ…ظˆط§ظ„', 'tag': 'O'}, {'token': 'ط§ط³طھ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'طھظ†ط¨ظ„', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'طھظ†â€Œظ¾ط±ظˆط±', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط\\xadط±ظپ', 'tag': 'O'}, {'token': 'ظ¾ط±ط³ط´', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾ط§غŒطھط®طھ', 'tag': 'O'}, {'token': 'ع©ط´ظˆط±', 'tag': 'O'}, {'token': 'ط¬ط²غŒط±ظ‡â€Œط§غŒ', 'tag': 'O'}, {'token': 'ظ†ط§ط¦ظˆط±ظˆ', 'tag': 'B-loc'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط§ظ‚غŒط§ظ†ظˆط³غŒظ‡', 'tag': 'B-loc'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¬ظ†ط³', 'tag': 'O'}, {'token': 'ط®ط´ظ†', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط³ط§ط²', 'tag': 'O'}, {'token': 'ط²ظ‡غŒ', 'tag': 'O'}, {'token': 'ط§غŒط±ط§ظ†غŒ', 'tag': 'B-org'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط¬ط§ظ‡ظ„غŒطھ', 'tag': 'O'}, {'token': 'ط¢ظ†', 'tag': 'O'}, {'token': 'ط±ط§', 'tag': 'O'}, {'token': 'ظ…غŒâ€Œظ¾ط±ط³طھغŒط¯ظ†ط¯', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط\\xadط§ط¬طھ', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ…ظ‚طµظˆط¯', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط²غŒط±ط§', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط¨ط±ط§غŒ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ†ظپغŒ', 'tag': 'O'}, {'token': 'طھط§ط²غŒ', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ط´طھ', 'tag': 'O'}, {'token': 'ظ¾ظ‡ظ†ط§ظˆط±', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ…ط±طھظپط¹', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'ط§طµط·ظ„ط§ط\\xad', 'tag': 'O'}, {'token': 'ط¬ط؛ط±ط§ظپغŒط§ط¦غŒ', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'طھط§ط±', 'tag': 'O'}, {'token': 'ط¹ظ†ع©ط¨ظˆطھ', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط¯ظˆط³طھط§ظ†', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط±ظپظ‚ط§', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ…ظ†ط§ظپظ‚', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ظ…ط²ط¯ظˆط±', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ…ط³ط§ظپط±', 'tag': 'O'}, {'token': 'ط³ط±ط²ظ…غŒظ†', 'tag': 'O'}, {'token': 'ط¹ط¬ط§ط¦ط¨', 'tag': 'O'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ظ¾ط±ط³ط´', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط®ط·', 'tag': 'O'}, {'token': 'ظ…ط§غŒظ„', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط§ط±غŒط¨', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط±ظˆط¯غŒ', 'tag': 'O'}, {'token': 'ط§ط³طھ', 'tag': 'O'}, {'token': 'ط¯ط±', 'tag': 'O'}, {'token': 'غŒظˆع¯ط³ظ„ط§ظˆغŒ', 'tag': 'B-loc'}, {'token': '0', 'tag': 'O'}, {'token': 'ظ€', 'tag': 'O'}, {'token': 'ط§ط²', 'tag': 'O'}, {'token': 'ظ…ط¹ط±ظˆظپطھط±غŒظ†', 'tag': 'O'}, {'token': 'ط¯ط§ط³طھط§ظ†ظ‡ط§غŒ', 'tag': 'O'}, {'token': 'طھط®غŒظ„غŒ', 'tag': 'O'}, {'token': 'عکظˆظ„', 'tag': 'B-pers'}, {'token': 'ظˆط±ظ†', 'tag': 'I-pers'}, {'token': 'ظ†ظˆغŒط³ظ†ط¯ظ‡', 'tag': 'O'}, {'token': 'ظپط±ط§ظ†ط³ظˆغŒ', 'tag': 'B-org'}, {'token': 'ظƒظ‡', 'tag': 'O'}, {'token': 'طھط§ع©ظ†ظˆظ†', 'tag': 'O'}, {'token': 'ع†ظ†ط¯غŒظ†', 'tag': 'O'}, {'token': 'ظپغŒظ„ظ…', 'tag': 'O'}, {'token': 'ظˆ', 'tag': 'O'}, {'token': 'ط³ط±غŒط§ظ„', 'tag': 'O'}, {'token': 'ط±ط§', 'tag': 'O'}, {'token': 'ظƒظ‡', 'tag': 'O'}, {'token': 'ط¨ط±', 'tag': 'O'}, {'token': 'ط§ط³ط§ط³', 'tag': 'O'}, {'token': 'ط¢ظ†', 'tag': 'O'}, {'token': 'ط³ط§ط®طھظ‡â€Œط§ظ†ط¯', 'tag': 'O'}, {'token': '.', 'tag': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "arman = ARMAN()\n",
    "print('**** Arman dataset **** ')\n",
    "print('splits: ', arman.info.splits)\n",
    "print(len(arman.train))\n",
    "print(next(arman.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\home\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - fasttext\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    fasttext-0.9.2             |   py39h2e52968_0         243 KB  esri\n",
      "    openssl-1.1.1n             |                0         2.2 MB  esri\n",
      "    pybind11-2.7.1             |                0         136 KB  esri\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _anaconda_depends  pkgs/main/win-64::_anaconda_depends-2022.05-py39_0\n",
      "  fasttext           esri/win-64::fasttext-0.9.2-py39h2e52968_0\n",
      "  pybind11           esri/win-64::pybind11-2.7.1-0\n",
      "  smart_open         pkgs/main/win-64::smart_open-5.2.1-py39haa95532_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2022.3.29-haa95532_1 --> 2022.07.19-haa95532_0\n",
      "  certifi                          2021.10.8-py39haa95532_2 --> 2022.6.15-py39haa95532_0\n",
      "  conda                               4.12.0-py39haa95532_0 --> 4.14.0-py39haa95532_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  openssl              pkgs/main::openssl-1.1.1n-h2bbff1b_0 --> esri::openssl-1.1.1n-0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  anaconda                                   2022.05-py39_0 --> custom-py39_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pybind11-2.7.1       | 136 KB    |            |   0% \n",
      "pybind11-2.7.1       | 136 KB    | ########## | 100% \n",
      "pybind11-2.7.1       | 136 KB    | ########## | 100% \n",
      "\n",
      "fasttext-0.9.2       | 243 KB    |            |   0% \n",
      "fasttext-0.9.2       | 243 KB    | ########## | 100% \n",
      "fasttext-0.9.2       | 243 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1n       | 2.2 MB    |            |   0% \n",
      "openssl-1.1.1n       | 2.2 MB    | ########## | 100% \n",
      "openssl-1.1.1n       | 2.2 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/win-64::anaconda==2022.05=py39_0\n",
      "  - defaults/win-64::gensim==4.1.2=py39hd77b12b_0\n"
     ]
    }
   ],
   "source": [
    "conda install -c esri fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fa_tokenizer exists in C:\\Users\\home\\.pernlp\\fa_tokenizer.pt\n",
      "2022-09-11 22:37:29,400 Cannot load model from c:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\saved_models/fa_tokenizer/fa_tokenizer.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\home\\\\anaconda3\\\\lib\\\\site-packages\\\\dadmatools\\\\saved_models/fa_tokenizer/fa_tokenizer.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\home\\Desktop\\NLP_Iternship\\Dadmatools\\dadmatoools.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# here lemmatizer and pos tagger will be loaded\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# as tokenizer is the default tool, it will be loaded even without calling\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# pips = 'lem,pos' \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# pips = 'dep' \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pips \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtok,lem,pos,dep,cons\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m nlp \u001b[39m=\u001b[39m language\u001b[39m.\u001b[39;49mPipeline(pips)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# you can see the pipeline with this code\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/home/Desktop/NLP_Iternship/Dadmatools/dadmatoools.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(nlp\u001b[39m.\u001b[39manalyze_pipes(pretty\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\pipeline\\language.py:258\u001b[0m, in \u001b[0;36mPipeline.__new__\u001b[1;34m(cls, pipeline)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, pipeline):\n\u001b[1;32m--> 258\u001b[0m     language \u001b[39m=\u001b[39m NLP(\u001b[39m'\u001b[39;49m\u001b[39mfa\u001b[39;49m\u001b[39m'\u001b[39;49m, pipeline)\n\u001b[0;32m    259\u001b[0m     nlp \u001b[39m=\u001b[39m language\u001b[39m.\u001b[39mnlp\n\u001b[0;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\pipeline\\language.py:64\u001b[0m, in \u001b[0;36mNLP.__init__\u001b[1;34m(self, lang, pipelines)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m# if 'def-norm' in pipelines:\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m#     global normalizer_model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m#     normalizer_model = normalizer.load_model()\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m#     self.nlp.add_pipe('normalizer', first=True)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mglobal\u001b[39;00m tokenizer_model\n\u001b[1;32m---> 64\u001b[0m tokenizer_model \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mload_model()\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m'\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[39mglobal\u001b[39;00m mwt_model\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\models\\tokenizer.py:125\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m mwt_dict \u001b[39m=\u001b[39m load_mwt_dict(args[\u001b[39m'\u001b[39m\u001b[39mmwt_json_file\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    124\u001b[0m use_cuda \u001b[39m=\u001b[39m args[\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args[\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 125\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model_file\u001b[39m=\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39msave_dir\u001b[39;49m\u001b[39m'\u001b[39;49m], use_cuda\u001b[39m=\u001b[39;49muse_cuda)\n\u001b[0;32m    126\u001b[0m loaded_args, vocab \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39margs, trainer\u001b[39m.\u001b[39mvocab\n\u001b[0;32m    128\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m loaded_args:\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\models\\tokenization\\trainer.py:19\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, args, vocab, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cuda \u001b[39m=\u001b[39m use_cuda\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# load everything from file\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[39m# build model from scratch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\dadmatools\\models\\tokenization\\trainer.py:85\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[0;32m     84\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(filename, \u001b[39mlambda\u001b[39;49;00m storage, loc: storage)\n\u001b[0;32m     86\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mCannot load model from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(filename))\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\home\\\\anaconda3\\\\lib\\\\site-packages\\\\dadmatools\\\\saved_models/fa_tokenizer/fa_tokenizer.pt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadmatools.embeddings import get_embedding, get_all_embeddings_info, get_embedding_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'glove',\n",
      " 'corpus': 'wikipedia',\n",
      " 'desc': 'source: https://github.com/Text-Mining',\n",
      " 'dim': 50,\n",
      " 'filename': 'vectors.txt',\n",
      " 'format': 'txt',\n",
      " 'url': 'https://raw.githubusercontent.com/Text-Mining/Persian-Wikipedia-Corpus/master/models/glove/vectors.zip'}\n"
     ]
    }
   ],
   "source": [
    "pprint(get_embedding_info('glove-wiki'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors.zip: 100%|██████████| 45.9M/45.9M [00:47<00:00, 1.01MB/s]\n",
      "[-0.308614 -0.168945 -2.576352  0.877447 -0.348502  0.582602  0.602845\n",
      "  0.471903  0.533526  0.906185  0.907475 -0.167968 -0.095735 -0.475923\n",
      "  0.276284  0.010084 -0.926263 -1.124971 -0.443414 -0.447227  0.259192\n",
      "  0.078348  0.916888 -0.061847 -0.853357  0.996823 -0.26386   0.621702\n",
      "  0.768682  0.250663  0.358242  0.571274 -0.321239  0.012563 -0.567481\n",
      "  0.560345 -0.206234 -0.187835 -0.665903  0.234979 -0.442619  0.164727\n",
      " -0.262    -0.172979 -0.393394 -0.474647  0.480312  1.106502  0.767303\n",
      "  0.046918]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding = get_embedding('glove-wiki')\n",
    "print(embedding['ابزار'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.2652763e-02  3.8418624e-01 -1.8762367e+00 -8.6866260e-02\n",
      "  3.6461627e-01  7.5215775e-01  3.6994025e-01  4.9959701e-01\n",
      "  1.2264743e-02  3.3335799e-01  5.5867076e-01  3.5873100e-01\n",
      "  4.2627126e-01 -8.8378501e-01 -1.2670399e-01 -7.0495725e-01\n",
      " -6.2538046e-01 -5.5862820e-01 -3.2012752e-01 -1.8887758e-02\n",
      "  2.8124401e-01  1.6167176e-01  5.9974694e-01  3.4806246e-01\n",
      " -1.4647543e-03  7.3103124e-01  1.9454075e-01  3.4274727e-01\n",
      "  5.1055348e-01  5.3316355e-01  5.8826029e-01  1.2634257e+00\n",
      " -1.2206910e+00 -4.0682977e-01 -2.4609923e-01  6.5093577e-01\n",
      " -2.5686526e-01 -4.0690476e-01  4.8100728e-01  4.8069999e-02\n",
      " -6.2497050e-01 -2.3815494e-02  2.1647224e-01 -2.1010575e-01\n",
      " -8.5227352e-01 -4.0755576e-01  8.1856251e-02  1.1975710e+00\n",
      "  5.1946604e-01  5.7960773e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding.embedding_text('ابزار پردازش متن فارسی'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7716714"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.similarity('کتاب', 'کتب')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7716714"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.similarity('کتاب', 'کتب')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('کتابی', 0.9353402256965637),\n",
       " ('کتاب\\u200cهای', 0.8594834208488464),\n",
       " ('جلد', 0.8522470593452454),\n",
       " ('تالیف', 0.8399883508682251),\n",
       " ('نوشته', 0.8382429480552673),\n",
       " ('مقاله', 0.8335504531860352),\n",
       " ('نوشته\\u200cاست', 0.8273731470108032),\n",
       " ('شرح', 0.8273376822471619),\n",
       " ('ترجمه', 0.8256694078445435),\n",
       " ('می\\u200cنویسد', 0.8014417886734009)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.top_nearest('کتاب', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4812294c6f66d1324ff60e8be9c61f62a5f209d02eb7146b1d2633c46157a481"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
